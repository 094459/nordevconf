from airflow.decorators import dag, task
from airflow.providers.amazon.aws.operators.athena import AthenaOperator
from airflow.providers.mysql.operators.mysql import MySqlOperator
from airflow.providers.mysql.transfers.s3_to_mysql import S3ToMySqlOperator
from datetime import timedelta, datetime
import requests
import json
import csv
import boto3

@dag(schedule_interval=timedelta(days=1), start_date=datetime(2023, 6, 1), catchup=False, description='The essential collection of bad jokes to keep me amused')
def my_funny_joke_archive_dag():

    # Define some variables - you would typically do this in the Airflow UI
    s3_bucket = "094459-jokes"
    athena_database = "my_joke_archive"
    athena_table_name = "best_jokes_table"

    time = datetime.now().strftime("%m/%d/%Y").replace('/', '-')
    csv_filename = f"jokes-{time}.csv"
    s3_csv_file= f"{time}/{csv_filename}"

    athena_query = """

 CREATE EXTERNAL TABLE IF NOT EXISTS {athena_database}.{athena_table_name} (
  category string,
  joke string,
  punchline string 
)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
LOCATION 's3://{s3_bucket}/'

TBLPROPERTIES (
  'has_encrypted_data'='false',
  'skip.header.line.count'='1'
) 
;

""".format(s3_bucket=s3_bucket, athena_table_name=athena_table_name, athena_database=athena_database)

    @task
    def pull_jokes():
        # pull jokes with the API
        url = r"https://official-joke-api.appspot.com/random_ten"
        response = requests.get(url)
        text = json.loads(response.text)

        # export to csv
        with open(csv_filename, 'w', newline='') as csv_file:
            csv_writer = csv.writer(csv_file)
            csv_writer.writerow(['Type', 'Setup', 'Punchline'])
            for i in text:
                csv_writer.writerow([i['type'], i['setup'], i['punchline']])
                print(i)

        # strip quotes
        with open(csv_filename, "r+", encoding="utf-8") as csv_file:
            content = csv_file.read()
        with open(csv_filename, "w+", encoding="utf-8") as csv_file:
            csv_file.write(content.replace('"', ''))

        # upload data_file to S3 bucket
        s3_client = boto3.client('s3')
        s3_client.upload_file(csv_filename, s3_bucket, s3_csv_file)
        print(f"File {csv_filename} uploaded to S3 bucket {s3_bucket}")

    @task
    def create_mysql_table():
        create_joke_table = """
            CREATE TABLE IF NOT EXISTS bad_jokes (
                category TEXT NOT NULL,
                joke TEXT NOT NULL,
                punchline TEXT NOT NULL
            );
        """
        return create_joke_table

    csv_generate_task = pull_jokes()
    create_joke_table_task = create_mysql_table()
    import_csv_to_athena = AthenaOperator(
        task_id='import_csv_to_athena',
        query=athena_query,
        database=athena_database,
        output_location=f's3://{s3_bucket}/import-processing/',
        aws_conn_id='aws_default',
    )
    export_csv_to_mysql_task = S3ToMySqlOperator(
        s3_source_key=f"s3://{s3_bucket}/{s3_csv_file}",
        mysql_table='bad_jokes',
        mysql_duplicate_key_handling='IGNORE',
        mysql_extra_options="""
            FIELDS TERMINATED BY ','
            IGNORE 1 LINES
        """,
        task_id='export_csv_to_mysql',
        aws_conn_id='aws_default',
        mysql_conn_id='mysql_default',
    )

    csv_generate_task >> import_csv_to_athena
    csv_generate_task >> export_csv_to_mysql_task
    export_csv_to_mysql_task >> import_csv_to_athena
    create_joke_table_task >> export_csv_to_mysql_task

my_funny_joke_archive_dag = my_funny_joke_archive_dag()
